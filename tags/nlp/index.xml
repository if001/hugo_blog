<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>nlp on アンドロイドは推理小説を書くか?</title>
    <link>https://www.if-blog.site/tags/nlp/</link>
    <description>Recent content in nlp on アンドロイドは推理小説を書くか?</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <atom:link href="/tags/nlp/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>SeqGANの論文を読む</title>
      <link>https://www.if-blog.site/post/nlp/seqgan-paper/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.if-blog.site/post/nlp/seqgan-paper/</guid>
      <description>SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient
https://arxiv.org/pdf/1609.05473.pdf</description>
    </item>
    
    <item>
      <title>ニューラルネットワークを用いた対話モデルのための多様性を促進する目的関数</title>
      <link>https://www.if-blog.site/post/nlp/diversity_neural_conversation_model/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.if-blog.site/post/nlp/diversity_neural_conversation_model/</guid>
      <description>Qiitaに投稿した記事、kerasでHREDを構築してみるの記事で、こちらの論文が参考になるとのコメント頂いて、読んで見たので簡単にまとめました。
A Diversity-Promoting Objective Function for Neural Conversation Models https://arxiv.</description>
    </item>
    
    <item>
      <title>文字をベクトル化する</title>
      <link>https://www.if-blog.site/post/nlp/char_vec/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.if-blog.site/post/nlp/char_vec/</guid>
      <description>文章生成にchar-level lstmを使ってみる。英語ではうまくいっている例があるが日本語では難しい。これは、日本語は英語に比べ文字数が多く、ニューラルネットワークの次元数(パラメータ数)が増やす必要があるのが原因の1つだと思う。また、次元削減のため、日本語では文章を単語に区切り単語をベクトル化し、lstmで文章を生成する手法もあるが、単語に区切る時点でしゃべり言葉やネットの言葉ではうまく区切れないという問題がある。そこで、日本語の文字を画像として生成し、その画像をauto-encoderを用いてベクトル化することで、文字のベクトル化を行い、lstmに食わせるという手法を試して見ようと思う。
今回は、auto-encodeを用いた文字レベルのベクトル化までを行ってみようと思う。
コードはここ、https://github.com/if001/fifc.git
以下の3工程で行う。
 フォントファイルからフォント画像を生成 文字列とフォント画像をマッピング フォント画像から特徴量を生成  フォントファイルからフォント画像を生成 フォントファイルは、PILのImageFontのturetypeを使い読み込むことができる。</description>
    </item>
    
    <item>
      <title>自然言語処理シリーズの構文解析を読む（概要）</title>
      <link>https://www.if-blog.site/post/nlp/nlp-parse-overview-1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.if-blog.site/post/nlp/nlp-parse-overview-1/</guid>
      <description>はじめに 自然言語処理シリーズの構文解析を読んでいきます。   全体の概要把握:1時間 中身の細かいとこ：3時間 という感じで読み進めて行こうと思います。
概要 構文解析を用いることで、単語の並びの背後にある文法的な構造を明らかにすることができる。構文解析を学ぶことで、自然言語処理で用いられる様々な先人の知恵を学習できる。</description>
    </item>
    
  </channel>
</rss>
